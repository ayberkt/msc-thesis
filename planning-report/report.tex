\documentclass{article}

\usepackage[utf8]{inputenc}

%% \usepackage{natbib}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{graphicx}

\usepackage{parskip}
\setlength{\parskip}{10pt} 

\usepackage{tikz}
\usetikzlibrary{arrows, decorations.markings}

\usepackage{chngcntr}
\counterwithout{figure}{section}

\begin{document}
\begin{titlepage}

\centering
  
{\scshape\LARGE Master thesis project planning report\\}
  
\vspace{0.5cm}
  
{\huge\bfseries Point-free Topology in Univalent Type Theory\\}
  
\vspace{2cm}
  
{\Large Ayberk Tosun (\texttt{ayberk@student.chalmers.se})\\}
  
\vspace{1.0cm}
  
{\large Suggested Supervisor at CSE: Thierry Coquand\\}
  
\vspace{1.5cm}
  
{\large Relevant completed courses:\\}
  
{\itshape (Exempted from) Logic in Computer Science\\}
{\itshape (Exempted from) Programming Language Technology\\}
{\itshape Types for Proofs and Programs\\}
{\itshape Models of Computation\\}
{\itshape Formal Methods in Software Engineering\\}
{\itshape Research-oriented Course in Domain Theory\\}
  
\vfill

\vfill
  
{\large \today\\} 

\end{titlepage}

\section{Introduction}

Construction of a semantic model for the untyped $\lambda$-calculus had eluded computer
scientists for quite some time when the first solution was given by Dana Scott in
1969~\cite{scott:1969}; he termed the mathematical structures he used to present a
denotational semantics for the $\lambda$-calculus \emph{domains}, hence giving rise to what is
now known as \emph{domain theory}.

The key insight in Scott's solution was the formulation of an \emph{information ordering}
between terms of the $\lambda$-calculus that orders terms with respect to ``how defined'' they
are. It turns out that considering functions that respect this information ordering in
various ways is crucial for the construction of a semantic model for the untyped
$\lambda$-calculus, and in fact for any programming language that is capable of general
recursion. The information ordering arose from an insight into the \emph{topological nature}
(in a sense that will be explained later) of the notion of partial information that
pervades the realm of computer science.

Scott initially formulated the theory of domains in the language of lattice
theory~\cite{birkhoff:1940}. Even though this approach enables a smooth development of the
required constructions (for purposes of semantics), it doesn't express the underlying
ideas in their full generality and involves the development of some technical machinery
that seems \emph{ad hoc} when taken out of context. To put it in Scott's
words~\cite[pg.~577]{scott:1982}:

\begin{quote}
  I feel I made a mistake in 1969 in using Lattice Theory as a mode of
  presentation---a mistake as far as Computer Science is concerned.
\end{quote}

It is for this reason that Scott tried another presentation in 1982~\cite{scott:1981}
formulating the notion of \emph{neighbourhood systems}, making a direct link with the
underlying topological insights. Even though this approach turned out to simplify the
presentation, Scott thought they left ``too much implicit'' and hence attempted yet
another formulation of domains, which he called \emph{information systems}.

Right around the time Scott first devised the theory of domains, a mathematical
development was also taking place: the theory of \emph{abstract} or \emph{point-free}
topological spaces in which one takes the notion of a ``set of open sets'' as a primordial
entity and carries out the development based on the algebraic essence of these abstract
opens. Although the pioneering development in this field was done by
Stone~\cite{stone:1934}, it was not until the 1970s that the study of abstract topological
spaces became a discipline of study in its own right with an important paper by John
Isbell~\cite{isbell:1973}.

\emph{Locale theory} emerged as a refinement of the pointless approach to topology~\cite{
johnstone:1982} and it was pointed out that it has direct connections to computer
science in the vein of research instigated by Scott and it turns out that the notion of
information systems was already leaning towards this topological view of computation.

Practically, we may say that locale theory gives us a way to algebraicize the notion of an
\emph{observable property of a program}: a property whose satisfaction an observer can
judge \emph{on the basis of finite information}. This idea builds upon the realization
that observable properties of programs behave like open sets in a topological space.

Abramsky~\cite{abramsky:1991} took advantage of locale theory for the elucidation and
further transformation of domain theory; as he explains \cite{abramsky:1991}, there are
two interpretations of Stone duality:
\begin{enumerate}
  \item the topological view: \emph{points vs. open sets}, and
  \item the logical view: \emph{models vs. formulas}.
\end{enumerate}
the third one that was put forth by Abramsky is:
\begin{center}
  the computer science view: computations vs. their specifications.
\end{center}

To put it more precisely: the fundamental relation of interest in software verification is

\begin{center}
  $P \vDash \phi$, \qquad read as, \qquad ``program $P$ satisfies property $\phi$''.
\end{center}

Stone duality allows us to understand this relationship better: the spatial (``pointful'')
side of the duality enables us to view types as topological spaces consisting of points
and programs as those points whereas the localic (``pointless'') side of the duality
provides a ``logical interpretation of denotational semantics'', as a proof-theoretic
system.

\section{Goals and Challenges}

In order to state the goal of the thesis project precisely, let us first start with the
development of some technical notions, starting with the question of: \emph{what is
  topology?}

Topology, as classically conceived, is the abstract study of nearness between points in
some space; it is abstract in the sense that the notion of nearness is formulated without
any reference to a measure of ``how near'' the points are.

Let $X$ be a set of points. A \emph{topology} on $X$ is a collection of subsets of
$\Omega(X) \subseteq P(X)$, called its \emph{open subsets}, that satisfy:
\begin{itemize}
  \item $X \in \Omega(X)$
  \item $O_1, O_2 \in \Omega(X)$, $O_1 \cap O_2 \in \Omega(X)$
  \item $O \subseteq \Omega(X)$, $\bigcup O \in \Omega(X)$.
\end{itemize}

A set $X$ equipped with a topology is called a \emph{topological space}. To move towards a
\emph{pointless} formulation of topology, we start by noticing that the inclusion relation
on the collection $\Omega(X)$ of open subsets is crucial and start by characterising it as an
abstract order. We take some set $O$ of opens and an ordering relation $\sqsubseteq \subseteq O \times O$ which
must be partial order that admits finite meets and arbitrary joins. Furthermore, the
following law is also validated in the pointful setting: given some $U \in \Omega(X)$,
$\mathcal{V} \subseteq \Omega(X)$
\begin{equation*}
  X \cap (\bigcup_{i} V_i) = \bigcup_{i} X \cap V_i
\end{equation*}
meaning we must also require that binary meets distribute over arbitrary joins. Such a
partially ordered set that admits all finite meets, all joins, and in which binary meets
distribute over joins is called a \emph{frame}. Frames form a category in which the
morphisms are frame homomorphisms. A \emph{locale} is an object in the opposite category
of the category of frames. So locales are identical to frames as long as no reference to
morphisms are made. Locales play a more important role that frames do

A powerful technique in pointless topology is that of a \emph{presentation} originating
from the realm of universal algebra. We start with an algebraic theory $F$ providing a
syntactic specification for an algebra. The presentation of an $F$-algebra $A$ is then a
pair of a set of \emph{generators} and a set of \emph{relations} such that $A$ generated
\emph{freely} by these generators and relations.

So how are locales related to logic exactly? The idea is that we can view locales as being
\emph{systems of finite observations} by viewing the set of opens as a set of observable
properties i.e. properties whose validity can be judged on the basis of a finite amount of
information. Once we start viewing locales in this way, the relevance of presentations
also becomes evident: this is a fundamental method in logic where one inductively
generates logical expressions via connectives and then identifies the expressions that
entail each other.

%% The connection to logic and computer science begins to appear upon the realisation that
%% frames can be viewed as \emph{systems of finite observations} by viewing each open as an
%% observable property i.e. a property whose validity can be judged on the basis of finite
%% information.

\section{Approach}

As previously mentioned, I will be using Cubical Agda~\cite{cubicalagda} which as an
extension of the Agda proof assistant~\cite{norell:2008}. Let us start by briefly
explaining what Cubical Type Theory is (or rather what it achieves).

The development of Homotopy Type Theory (HoTT)~\cite{hottbook} has been a substantial
advancement throughout the history of type theory if not all of mathematical foundations.
From the practical point of view, HoTT provides the user with access to the following
technical advantages that revolve around the \emph{univalence axiom}. For the sake of
brevity, we refrain from engaging in a technical explanation of the univalence axiom and
instead focus on the more practical applications:

\begin{itemize}
  \item \emph{Function extensionality}. It is extremely common for Agda users to postulate
    function extensionality which is not a theorem of type theory. However, it was proven
    by Voevodsky that function extensionality is a theorem in a univalent setting.
  \item \emph{Homotopy levels}. TODO: explain.
  \item \emph{Higher inductive types}. Normally inductive types are defined by defining
    what inhabitants they have. The notion of a higher inductive type extends this idea by
    allowing the user to add \emph{equalities} (i.e., new inhabitants of the identity
    type) in the type therefore identifying some of the elements.
\end{itemize}

A severe shortcoming of HoTT has been the fact that univalence needs to postulated. In
light of the view of type theory as a practical programming language, this means that
\emph{we do not know how to run programs that make use of univalence}.

The computational meaning of univalence was came to light with by the development of
\emph{cubical type theory}~\cite{cubicaltt} in which the univalence axiom is a theorem.
This is an amazing result in a sense in that it allows a programmer to prove insertion
sort (extensionally) equal to quicksort and then a develop a theory of their programs that
use quicksort as though they used insertion sort.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
